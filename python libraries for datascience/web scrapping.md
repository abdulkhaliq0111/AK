
# Web scrapping

 Here are useful Python libraries for extracting and collecting data.

**Beautiful soup**

BeautifulSoup is beginner-friendly and fairly powerful aimed at helping programmers who are trying to scrape data from websites.
Beautiful Soup sits on top of popular Python parsers like lxml and html5lib. Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8

**Scrapy**

A fast and powerful open source web crawling framework for extracting structured data you need from websites that be used for a wide range of useful applications, like data mining, information processing, or historical archival.
Go for Scrapy if your motive is fast, high-level screen scraping & crawling.
One of the main advantages of Scrapy: requests are scheduled and processed asynchronously.

[gain](https://github.com/gaojiuli/gain) 

Web crawling framework based on asyncio.


[MechanicalSoup](https://github.com/MechanicalSoup/MechanicalSoup) 

A Python library for automating interaction with websites.

[camelot](https://github.com/socialcopsdev/camelot) 

Camelot: PDF Table Extraction for Humans.


**Lxml**

Lxml is a Python library which allows for easy handling of XML and HTML files, and can also be used for web scraping. The key benefits of this library are that its ease of use, extremely fast when parsing large documents, very well documented, and provides easy conversion of data to Python data types, resulting in easier file manipulation.

**Requests**

The request is a Python library that let you send HTTP/1.1 request, requests, add headers, form data, multipart files, & parameters with simple Python dictionaries.
It abstracts the complexities of making requests behind a beautiful, simple API so that you can focus on interacting with services and consuming data in your application

[sum](https://github.com/miso-belica/sumy) 

Automatic summarization of text documents and HTML.

[textract](https://github.com/deanmalmgren/textract) 

Extract text from any document.

**Selenium**

Selenium Library is a web testing library for Robot Framework that utilizes the Selenium tool internally. Primarily it is for automating web applications for testing purposes but is certainly not limited to just that. Boring web-based administration tasks can (and should) be automated as well.

**Wget**

Wget is a free utility for non-interactive downloading files from the web. It supports HTTP, HTTPS, and FTP protocols, as well as retrieval through HTTP proxies. Since it is non-interactive, it can work in the background even if the user isn't logged in.

**Mechanize**

A very useful python module for navigating through web forms is Mechanize.  It gives you a browser like an object to interact with web pages.

[newspaper](https://github.com/codelucas/newspaper) 

News extraction, article extraction, and content curation.



```python

```

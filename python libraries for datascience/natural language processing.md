
# NATURAL LANGUAGE PROCESSING 

Here are useful Python libraries for Natural Language Processing.

[NLTK](https://www.nltk.org) - A leading platform for building Python programs to work with human language data.

[jieba](https://github.com/fxsjy/jieba#jieba-1) - Accurate Mode attempts to cut the sentence into the most accurate segmentation, which is suitable for text analysis.

[SnowNLP](https://github.com/isnowfy/snownlp) - A library for processing Chinese text.

[TextBlob](http://textblob.readthedocs.io/en/dev/) - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.

[YAlign](https://github.com/machinalis/yalign) - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora.

[nut](https://github.com/pprett/nut) - Natural language Understanding Toolkit.

[Rosetta](https://github.com/columbia-applied-data-science/rosetta) - Text processing tools and wrappers (e.g. Vowpal Wabbit)

[spammy](https://github.com/tasdikrahman/spammy) - A library for email Spam filtering built on top of nltk.it trains the classifier on your dataset to classify your emails into spam or ham

[loso](https://github.com/fangpenlin/loso) - Another Chinese segmentation library.Loso determines segmentation according to the lexicon database, and the algorithm is based on the Hidden Markov Model, therefore, it is not possible to use the service before building a lexicon database.

[genius](https://github.com/duanhongyi/genius) - A Chinese segment base on Conditional Random Field.

[KoNLPy](http://konlpy.org) - A Python package for Korean natural language processing.

[Pattern](https://www.clips.uantwerpen.be/pattern) - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.

[Quepy](https://github.com/machinalis/quepy) - A python framework to transform natural language questions to queries in a database query language.

[BLLIP Parser](https://pypi.org/project/bllipparser/) - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).

[colibri-core](https://github.com/proycon/colibri-core) - Python binding to C++ library for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.

[spaCy](https://github.com/explosion/spaCy) - Industrial strength NLP with Python and Cython.Spacy has integrated word vectors and a fast & accurate part-of-speech tagger + dependency parser. If you want something that has good defaults, Spacy is the way to go.

[PyStanfordDependencies](https://github.com/dmcc/PyStanfordDependencies) - Python interface for converting Penn Treebank trees to Stanford Dependencies.

[Distance](https://github.com/doukremt/distance) - Levenshtein and Hamming distance computation.

[pkuseg-python](https://github.com/lancopku/pkuseg-python) - A better version of Jieba, developed by Peking University.

[Fuzzy Wuzzy](https://github.com/seatgeek/fuzzywuzzy) -Fuzzy string matching is the process of finding strings that match a given pattern. It uses Levenshtein Distance to calculate the differences between sequences.

[jellyfish](https://github.com/jamesturk/jellyfish) - a python library for doing approximate and phonetic matching of strings.

[CLTK](https://github.com/cltk/cltk) - The Classical Language Toolkit.

[rasa_nlu](https://github.com/RasaHQ/rasa_nlu) - turn natural language into structured data.

[editdistance](https://pypi.org/project/editdistance/) - fast implementation of edit distance.edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other.

[textacy](https://github.com/chartbeat-labs/textacy) - higher-level NLP built on Spacy.

[PyNLPl](https://github.com/proycon/pynlpl) - Python Natural Language Processing Library. General-purpose NLP library for Python. It also contains some specific modules for parsing common NLP formats, most notably for FoLiA, but also ARPA language models, Moses phrase tables, GIZA++ alignments.

[stanford-corenlp-python](https://github.com/stanfordnlp/CoreNLP) - Python wrapper for Stanford CoreNLP.

[yase](https://github.com/PPACI/yase) - Yase enables you to encode any sequence which can be represented by string to be encoded into a list of word-vector representation.

[Polyglot](https://github.com/aboSamoor/polyglot) - Multilingual text (NLP) processing toolkit.

[DrQA](https://github.com/facebookresearch/DrQA) - Reading Wikipedia to answer open-domain questions.

[Dedupe](https://github.com/dedupeio/dedupe) - A python library for accurate and scalable fuzzy matching, record deduplication, and entity-resolution.

[Snips NLU](https://github.com/snipsco/snips-nlu) - Natural Language Understanding library for intent classification and entity extraction

[NeuroNER](https://github.com/Franck-Dernoncourt/NeuroNER) - Named-entity recognition using neural networks providing state-of-the-art-results

[DeepPavlov](https://github.com/deepmipt/DeepPavlov/) - conversational AI library with many pretrained Russian NLP models.

[BigARTM](https://github.com/bigartm/bigartm) - topic modelling platform.

[PySS3](https://github.com/sergioburdisso/pyss3) - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 can visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools.

[python-ucto](https://github.com/proycon/python-ucto) - Python binding to ucto (a Unicode-aware rule-based tokenizer for various languages).

[python-frog](https://github.com/proycon/python-frog) - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatization, dependency parsing, NER)

[python-zpar](https://github.com/EducationalTestingService/python-zpar) - Python bindings for ZPar, a statistical part-of-speech-tagger, constituency parser, and dependency parser for English.
